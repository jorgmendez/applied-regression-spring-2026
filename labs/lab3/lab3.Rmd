---
title: "Lab 3"
author: "Jorge Mendez"
date: "2026-02-07"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)

```

## Problem 1:  savings (faraway)

### (1a) Build the Design Matrix $X$ and Compute $\hat{\boldsymbol{\beta}}$ from Scratch

We consider the multiple linear regression model
\[
\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon},
\qquad
\boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma^2 I).
\]

Here, $\mathbf{y}$ is the response vector, $X$ is the design (model) matrix containing a column of ones for the intercept and the predictor variables, $\boldsymbol{\beta}$ is the vector of regression coefficients, and $\boldsymbol{\varepsilon}$ is the random error vector.

The ordinary least squares (OLS) estimator of $\boldsymbol{\beta}$ is given by
\[
\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T \mathbf{y}.
\]

This estimator minimizes the sum of squared residuals and provides the regression coefficient estimates for the multiple linear regression model.

```{r}
library(faraway)
data(savings)

# response
y <- savings$sr

# predictors (everything except sr)
X_pred <- savings[, names(savings) != "sr"]

# design matrix (add intercept column of 1s)
X <- cbind(Intercept = 1, as.matrix(X_pred))

# beta-hat from scratch
beta_hat <- solve(t(X) %*% X) %*% (t(X) %*% y)

# show first rows of X and the estimates
head(X)
beta_hat

```
From the output above, the design matrix $X$ consists of a column of ones corresponding to the intercept and the predictor variables $\text{pop15}$, $\text{pop75}$, $\text{dpi}$, and $\text{ddpi}$. Using the matrix formula
\[
\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T \mathbf{y},
\]
we obtain the estimated regression coefficients
\[
\hat{\boldsymbol{\beta}} =
(28.5661,\,-0.4612,\,-1.6915,\,-0.000337,\,0.4097)^T.
\]
These estimates represent the fitted effects of each predictor on the saving rate $sr$ and agree with the coefficients produced by the \texttt{lm()} function in \textsf{R}.


### (1b) Fit the Multiple Linear Regression Model

```{r}
# Fit the multiple linear regression model
model <- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = savings)

# Model summary
summary(model)
```

The fitted multiple linear regression model is
\[
\widehat{sr}
=
28.57
- 0.461\,\text{pop15}
- 1.691\,\text{pop75}
- 0.000337\,\text{dpi}
+ 0.410\,\text{ddpi}.
\]

Based on the coefficient $t$-tests, $\text{pop15}$ (p-value $0.0026$) and $\text{ddpi}$ (p-value $0.0425$) are statistically significant predictors of the saving rate at the $\alpha = 0.05$ level, while $\text{pop75}$ and $\text{dpi}$ are not. The overall regression is statistically significant with $F = 5.756$ and p-value $0.00079$. The coefficient of determination is $R^2 = 0.3385$, indicating that approximately $33.85\%$ of the variability in the saving rate is explained by the model.

### (1c) ANOVA procedure: compute SST, SSR, SSE

```{r}
aov_tab <- anova(model)
aov_tab

```
```{r}
# degrees of freedom
p  <- 4
df2 <- 45

# Sum of Squares from ANOVA table
SSR <- 204.12 + 53.34 + 12.40 + 63.05
SSE <- 650.71
SST <- SSR + SSE

# F-statistic
F_stat <- (SSR/p) / (SSE/df2)

# p-value
p_value <- 1 - pf(F_stat, p, df2)

c(SST = SST, SSR = SSR, SSE = SSE, F = F_stat, p_value = p_value)

```
From the ANOVA table, the regression sum of squares is
\[
SSR = 204.12 + 53.34 + 12.40 + 63.05 = 332.91,
\]
and the error sum of squares is
\[
SSE = 650.71.
\]
Thus, the total sum of squares is
\[
SST = SSR + SSE = 983.62.
\]

The ANOVA $F$-statistic is
\[
F
=
\frac{SSR/p}{SSE/(n-p-1)}
=
\frac{332.91/4}{650.71/45}
=
5.756,
\]
with degrees of freedom $(4,45)$ and p-value $0.00079$.

Since the p-value is less than $0.05$, we reject the null hypothesis that all slope coefficients are zero. Therefore, the regression model is statistically significant.


### (1c) Reduced model + partial F test

```{r}
# Reduced model
model.red <- lm(sr ~ pop15 + ddpi, data = savings)

summary(model.red)

# Partial F-test: reduced vs full
anova(model.red, model)


```
### (1d) Reduced Model and Partial $F$-Test

From part (1b), the predictors $\text{pop75}$ and $\text{dpi}$ were not statistically significant at the $\alpha = 0.05$ level. We therefore consider a reduced model obtained by removing these variables.

The reduced model is
\[
sr = \beta_0 + \beta_1\,\text{pop15} + \beta_2\,\text{ddpi} + \varepsilon,
\qquad
\varepsilon \sim N(0,\sigma^2).
\]

To compare the reduced model with the full model, a partial $F$-test was performed with hypotheses
\[
H_0:\ \beta_{\text{pop75}} = \beta_{\text{dpi}} = 0
\quad \text{vs.} \quad
H_a:\ \text{at least one of these coefficients is nonzero}.
\]

The partial $F$-test yields
\[
F = 1.72 \quad \text{with p-value } 0.19.
\]

Since the p-value exceeds $0.05$, we fail to reject $H_0$. Therefore, removing $\text{pop75}$ and $\text{dpi}$ does not significantly reduce the explanatory power of the model, and the reduced model is preferred for its simplicity.


### (1e)  Perform 1 prediction

```{r}
# Choose one value for pop15 (example: 35)
newdata <- data.frame(
  pop15 = 35,
  pop75 = mean(savings$pop75),
  dpi   = mean(savings$dpi),
  ddpi  = mean(savings$ddpi)
)

newdata



```


```{r}
pred <- predict(model, newdata, se.fit = TRUE)
pred

```

```{r}
tcrit <- qt(0.975, df = pred$df)
half_width <- tcrit * pred$se.fit

CI <- c(
  fit = pred$fit,
  lwr = pred$fit - half_width,
  upr = pred$fit + half_width
)

CI

```
Using the fitted multiple linear regression model, we predict the saving rate when
$\text{pop15} = 35$ and the remaining predictors are held at their sample means.
The estimated mean saving rate is
\[
\widehat{sr} = 9.71.
\]

A 95\% confidence interval for the mean saving rate is
\[
(8.63,\ 10.80).
\]


## Problem 2

```{r}
# NFL data: Table B.1 (1976 season)

table.b1 <- data.frame(
  y  = c(10,11,11,13,10,11,10,11,4,2,7,10,9,9,6,5,5,5,6,4,3,3,4,10,6,8,2,0),
  x2 = c(1985,2855,1737,2905,1666,2927,2341,2737,1414,1838,1480,2191,
         2229,2204,2140,1730,2072,2929,2268,1983,1792,1606,1492,2835,
         2416,1638,2649,1503),
  x7 = c(59.7,55.0,65.6,61.4,66.1,61.0,66.1,58.0,57.0,58.9,67.5,57.2,
         58.8,58.6,59.2,54.4,49.6,54.3,58.7,51.7,61.9,52.7,57.8,59.7,
         54.9,65.3,43.8,53.5),
  x8 = c(2205,2096,1847,1903,1457,1848,1564,1821,2577,2476,1984,1917,
         1761,1709,1901,2288,2072,2861,2411,2289,2203,2592,2053,1979,
         2048,1786,2876,2560)
)

head(table.b1)

```

### (2a) Fit the multiple linear regression model

We fit a multiple linear regression model relating the number of games won ($y$) to the team's passing yardage ($x_2$), the percentage of rushing plays ($x_7$), and the opponents' rushing yards ($x_8$):
\[
y = \beta_0 + \beta_2 x_2 + \beta_7 x_7 + \beta_8 x_8 + \varepsilon,
\qquad
\varepsilon \sim N(0,\sigma^2).
\]


```{r}
model2 <- lm(y ~ x2 + x7 + x8, data = table.b1)
summary(model2)

```
A multiple linear regression model was fit relating the number of games won ($y$) to passing yardage ($x_2$), percentage of rushing plays ($x_7$), and opponents' rushing yards ($x_8$). The fitted regression equation is
\[
\hat{y}
=
-1.81
+ 0.00360\,x_2
+ 0.194\,x_7
- 0.00482\,x_8.
\]

The estimated coefficient for passing yardage ($x_2$) is positive and statistically significant ($t = 5.18$, $p < 0.001$), indicating that teams with higher passing yardage tend to win more games. The coefficient for percent rushing plays ($x_7$) is also positive and significant ($t = 2.20$, $p = 0.038$), suggesting that a higher proportion of rushing plays is associated with more wins. The coefficient for opponents' rushing yards ($x_8$) is negative and significant ($t = -3.77$, $p < 0.001$), indicating that allowing more rushing yards is associated with fewer games won.


The overall $F$-test yields
\[
F = 29.44 \quad \text{with p-value } 3.27 \times 10^{-8},
\]
providing strong evidence that at least one of the predictors is linearly related to the number of games won. The coefficient of determination is $R^2 = 0.7863$, indicating that approximately $78.63\%$ of the variability in games won is explained by the model.


### (2b) ANOVA

```{r}
anova(model2)

```

The ANOVA table yields an overall $F$-statistic of
\[
F = 29.44 \quad \text{with p-value } 3.27 \times 10^{-8}.
\]
Since the p-value is much smaller than $0.05$, we reject the null hypothesis. Therefore, there is strong evidence that the regression model is significant and that at least one of the predictors is related to the number of games won.


### (2c) t-tests for B1, B7, B8

To test the contribution of each predictor, we test
\[
H_0:\ \beta_j = 0
\quad \text{vs.} \quad
H_a:\ \beta_j \neq 0,
\]
using the test statistic
\[
t = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)} \sim t_{n-p-1}.
\]
Here, $n=28$ and $p=3$, so the degrees of freedom are $n-p-1=24$.


```{r}
summary(model2)

```
From the fitted model output, the $t$-statistics are
\[
t_{x_2} = 5.177 \ (p = 2.66\times 10^{-5}),\quad
t_{x_7} = 2.198 \ (p = 0.037815),\quad
t_{x_8} = -3.771 \ (p = 0.000938).
\]
At the $\alpha = 0.05$ level, all three predictors $x_2$, $x_7$, and $x_8$ are statistically significant. Thus, passing yardage ($x_2$) and percent rushing plays ($x_7$) have significant positive associations with games won, while opponents' rushing yards ($x_8$) has a significant negative association with games won.





