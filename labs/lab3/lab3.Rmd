---
title: "Lab 3"
author: "Jorge Mendez"
date: "2026-02-07"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)

```

## Problem 1:  savings (faraway)

### (1a) Build the Design Matrix $X$ and Compute $\hat{\boldsymbol{\beta}}$ from Scratch

We consider the multiple linear regression model
\[
\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon},
\qquad
\boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma^2 I).
\]

Here, $\mathbf{y}$ is the response vector, $X$ is the design (model) matrix containing a column of ones for the intercept and the predictor variables, $\boldsymbol{\beta}$ is the vector of regression coefficients, and $\boldsymbol{\varepsilon}$ is the random error vector.

The ordinary least squares (OLS) estimator of $\boldsymbol{\beta}$ is given by
\[
\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T \mathbf{y}.
\]

This estimator minimizes the sum of squared residuals and provides the regression coefficient estimates for the multiple linear regression model.

```{r, echo=FALSE}
library(faraway)
data(savings)

# response
y <- savings$sr

# predictors (everything except sr)
X_pred <- savings[, names(savings) != "sr"]

# design matrix (add intercept column of 1s)
X <- cbind(Intercept = 1, as.matrix(X_pred))

# beta-hat from scratch
beta_hat <- solve(t(X) %*% X) %*% (t(X) %*% y)

# show first rows of X and the estimates
head(X)
beta_hat

```
From the output above, the design matrix $X$ consists of a column of ones corresponding to the intercept and the predictor variables $\text{pop15}$, $\text{pop75}$, $\text{dpi}$, and $\text{ddpi}$. Using the matrix formula
\[
\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T \mathbf{y},
\]
we obtain the estimated regression coefficients
\[
\hat{\boldsymbol{\beta}} =
(28.5661,\,-0.4612,\,-1.6915,\,-0.000337,\,0.4097)^T.
\]
These estimates represent the fitted effects of each predictor on the saving rate $sr$ and agree with the coefficients produced by the \texttt{lm()} function in \textsf{R}.


### (1b) Fit the Multiple Linear Regression Model

```{r}
# Fit the multiple linear regression model
model <- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = savings)

# Model summary
summary(model)
```

The fitted multiple linear regression model is
\[
\widehat{sr}
=
28.57
- 0.461\,\text{pop15}
- 1.691\,\text{pop75}
- 0.000337\,\text{dpi}
+ 0.410\,\text{ddpi}.
\]

Based on the coefficient $t$-tests, $\text{pop15}$ (p-value $0.0026$) and $\text{ddpi}$ (p-value $0.0425$) are statistically significant predictors of the saving rate at the $\alpha = 0.05$ level, while $\text{pop75}$ and $\text{dpi}$ are not. The overall regression is statistically significant with $F = 5.756$ and p-value $0.00079$. The coefficient of determination is $R^2 = 0.3385$, indicating that approximately $33.85\%$ of the variability in the saving rate is explained by the model.

### (1c) ANOVA procedure: compute SST, SSR, SSE

```{r}
aov_tab <- anova(model)
aov_tab

```
```{r}
# degrees of freedom
p  <- 4
df2 <- 45

# Sum of Squares from ANOVA table
SSR <- 204.12 + 53.34 + 12.40 + 63.05
SSE <- 650.71
SST <- SSR + SSE

# F-statistic
F_stat <- (SSR/p) / (SSE/df2)

# p-value
p_value <- 1 - pf(F_stat, p, df2)

c(SST = SST, SSR = SSR, SSE = SSE, F = F_stat, p_value = p_value)

```
From the ANOVA table, the regression sum of squares is
\[
SSR = 204.12 + 53.34 + 12.40 + 63.05 = 332.91,
\]
and the error sum of squares is
\[
SSE = 650.71.
\]
Thus, the total sum of squares is
\[
SST = SSR + SSE = 983.62.
\]

The ANOVA $F$-statistic is
\[
F
=
\frac{SSR/p}{SSE/(n-p-1)}
=
\frac{332.91/4}{650.71/45}
=
5.756,
\]
with degrees of freedom $(4,45)$ and p-value $0.00079$.

Since the p-value is less than $0.05$, we reject the null hypothesis that all slope coefficients are zero. Therefore, the regression model is statistically significant.


### (1d) Reduced Model and Partial $F$-Test

```{r}
# Reduced model
model.red <- lm(sr ~ pop15 + ddpi, data = savings)

summary(model.red)

# Partial F-test: reduced vs full
anova(model.red, model)


```


From part (1b), the predictors $\text{pop75}$ and $\text{dpi}$ were not statistically significant at the $\alpha = 0.05$ level. We therefore consider a reduced model obtained by removing these variables.

The reduced model is
\[
sr = \beta_0 + \beta_1\,\text{pop15} + \beta_2\,\text{ddpi} + \varepsilon,
\qquad
\varepsilon \sim N(0,\sigma^2).
\]

To compare the reduced model with the full model, a partial $F$-test was performed with hypotheses
\[
H_0:\ \beta_{\text{pop75}} = \beta_{\text{dpi}} = 0
\quad \text{vs.} \quad
H_a:\ \text{at least one of these coefficients is nonzero}.
\]

The partial $F$-test yields
\[
F = 1.72 \quad \text{with p-value } 0.19.
\]

Since the p-value exceeds $0.05$, we fail to reject $H_0$. Therefore, removing $\text{pop75}$ and $\text{dpi}$ does not significantly reduce the explanatory power of the model, and the reduced model is preferred for its simplicity.


### (1e)  Perform 1 prediction

```{r}
# Choose one value for pop15 (example: 35)
newdata <- data.frame(
  pop15 = 35,
  pop75 = mean(savings$pop75),
  dpi   = mean(savings$dpi),
  ddpi  = mean(savings$ddpi)
)

newdata



```


```{r}
pred <- predict(model, newdata, se.fit = TRUE)
pred

```

```{r}
tcrit <- qt(0.975, df = pred$df)
half_width <- tcrit * pred$se.fit

CI <- c(
  fit = pred$fit,
  lwr = pred$fit - half_width,
  upr = pred$fit + half_width
)

CI

```
Using the fitted multiple linear regression model, we predict the saving rate when
$\text{pop15} = 35$ and the remaining predictors are held at their sample means.
The estimated mean saving rate is
\[
\widehat{sr} = 9.71.
\]

A 95\% confidence interval for the mean saving rate is
\[
(8.63,\ 10.80).
\]


## Problem 2

```{r, echo=FALSE}
# NFL data: Table B.1 (1976 season)

table.b1 <- data.frame(
  y  = c(10,11,11,13,10,11,10,11,4,2,7,10,9,9,6,5,5,5,6,4,3,3,4,10,6,8,2,0),
  x2 = c(1985,2855,1737,2905,1666,2927,2341,2737,1414,1838,1480,2191,
         2229,2204,2140,1730,2072,2929,2268,1983,1792,1606,1492,2835,
         2416,1638,2649,1503),
  x7 = c(59.7,55.0,65.6,61.4,66.1,61.0,66.1,58.0,57.0,58.9,67.5,57.2,
         58.8,58.6,59.2,54.4,49.6,54.3,58.7,51.7,61.9,52.7,57.8,59.7,
         54.9,65.3,43.8,53.5),
  x8 = c(2205,2096,1847,1903,1457,1848,1564,1821,2577,2476,1984,1917,
         1761,1709,1901,2288,2072,2861,2411,2289,2203,2592,2053,1979,
         2048,1786,2876,2560)
)

head(table.b1)

```

### (2a) Fit the multiple linear regression model

We fit a multiple linear regression model relating the number of games won ($y$) to the team's passing yardage ($x_2$), the percentage of rushing plays ($x_7$), and the opponents' rushing yards ($x_8$):
\[
y = \beta_0 + \beta_2 x_2 + \beta_7 x_7 + \beta_8 x_8 + \varepsilon,
\qquad
\varepsilon \sim N(0,\sigma^2).
\]


```{r}
model2 <- lm(y ~ x2 + x7 + x8, data = table.b1)
summary(model2)

```
A multiple linear regression model was fit relating the number of games won ($y$) to passing yardage ($x_2$), percentage of rushing plays ($x_7$), and opponents' rushing yards ($x_8$). The fitted regression equation is
\[
\hat{y}
=
-1.81
+ 0.00360\,x_2
+ 0.194\,x_7
- 0.00482\,x_8.
\]

The estimated coefficient for passing yardage ($x_2$) is positive and statistically significant ($t = 5.18$, $p < 0.001$), indicating that teams with higher passing yardage tend to win more games. The coefficient for percent rushing plays ($x_7$) is also positive and significant ($t = 2.20$, $p = 0.038$), suggesting that a higher proportion of rushing plays is associated with more wins. The coefficient for opponents' rushing yards ($x_8$) is negative and significant ($t = -3.77$, $p < 0.001$), indicating that allowing more rushing yards is associated with fewer games won.


The overall $F$-test yields
\[
F = 29.44 \quad \text{with p-value } 3.27 \times 10^{-8},
\]
providing strong evidence that at least one of the predictors is linearly related to the number of games won. The coefficient of determination is $R^2 = 0.7863$, indicating that approximately $78.63\%$ of the variability in games won is explained by the model.


### (2b) ANOVA

```{r}
anova(model2)

```

The ANOVA table yields an overall $F$-statistic of
\[
F = 29.44 \quad \text{with p-value } 3.27 \times 10^{-8}.
\]
Since the p-value is much smaller than $0.05$, we reject the null hypothesis. Therefore, there is strong evidence that the regression model is significant and that at least one of the predictors is related to the number of games won.


### (2c) t-tests for B1, B7, B8

To test the contribution of each predictor, we test
\[
H_0:\ \beta_j = 0
\quad \text{vs.} \quad
H_a:\ \beta_j \neq 0,
\]
using the test statistic
\[
t = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)} \sim t_{n-p-1}.
\]
Here, $n=28$ and $p=3$, so the degrees of freedom are $n-p-1=24$.


```{r}
summary(model2)

```
From the fitted model output, the $t$-statistics are
\[
t_{x_2} = 5.177 \ (p = 2.66\times 10^{-5}),\quad
t_{x_7} = 2.198 \ (p = 0.037815),\quad
t_{x_8} = -3.771 \ (p = 0.000938).
\]
At the $\alpha = 0.05$ level, all three predictors $x_2$, $x_7$, and $x_8$ are statistically significant. Thus, passing yardage ($x_2$) and percent rushing plays ($x_7$) have significant positive associations with games won, while opponents' rushing yards ($x_8$) has a significant negative association with games won.



### (2e) Reduced model + partial F-test

```{r}
model2.red <- lm(y ~ x2 + x8, data = table.b1)
summary(model2.red)

```
```{r}
anova(model2.red, model2)

```
To assess the contribution of $x_7$ (percentage of rushing plays), we compare the full model
\[
y = \beta_0 + \beta_2 x_2 + \beta_7 x_7 + \beta_8 x_8 + \varepsilon
\]
with the reduced model
\[
y = \beta_0 + \beta_2 x_2 + \beta_8 x_8 + \varepsilon.
\]

The hypotheses for the partial $F$-test are
\[
H_0:\ \beta_7 = 0
\quad \text{vs.} \quad
H_a:\ \beta_7 \neq 0.
\]

The partial $F$-test yields
\[
F = 4.83 \quad \text{with p-value } 0.0378.
\]
Since the p-value is less than $0.05$, we reject the null hypothesis. Therefore, $x_7$ provides a statistically significant contribution to the model and should be retained.

## Problem 3

We fit the multiple linear regression model
\[
y = \beta_0 + \beta_1 x_1 + \beta_6 x_6 + \varepsilon,
\qquad
\varepsilon \sim N(0,\sigma^2),
\]
where $y$ is gasoline mileage (miles per gallon), $x_1$ is engine displacement (cubic inches), and $x_6$ is the number of carburetor barrels.


```{r, echo=FALSE}
# Table B.3 (only variables needed for Problem 3.5a): y, x1, x6
b3 <- data.frame(
  y  = c(18.90,17.00,20.00,18.25,20.07,11.20,22.12,21.47,34.70,30.40,16.50,36.50,
         21.50,19.70,20.30,17.80,14.39,14.89,17.80,16.41,23.54,21.47,16.59,31.90,
         29.40,13.27,23.90,19.73,13.90,13.27,13.77,16.50),
  x1 = c(350,350,250,351,225,440,231,262,89.7,96.9,350,85.3,
         171,258,140,302,500,440,350,318,231,360,400,96.9,
         140,460,133.6,318,351,351,360,350),
  x6 = c(4,4,1,2,1,4,2,2,2,2,4,2,
         2,1,2,2,4,4,4,2,2,2,4,2,
         2,4,2,2,2,2,4,4)
)

head(b3)

```

### (3a) Fit the regression model

```{r}
model3 <- lm(y ~ x1 + x6, data = b3)
summary(model3)

```

\[
y = \beta_0 + \beta_1 x_1 + \beta_6 x_6 + \varepsilon,
\qquad \varepsilon \sim N(0,\sigma^2).
\]

The fitted regression model is
\[
\widehat{y}
=
32.91
- 0.0530\,x_1
+ 0.9295\,x_6.
\]

### (3a) ANOVA 

```{r}
anova(model3)

```


\subsection*{(b) Analysis of Variance and Significance of Regression}

The analysis-of-variance (ANOVA) table for the fitted model yields an overall
$F$-statistic of
\[
F = 53.31 \quad \text{with p-value } 1.93 \times 10^{-10}.
\]
Since the p-value is far smaller than $0.05$, we reject the null hypothesis that
all slope coefficients are zero. Therefore, there is strong evidence that the
regression model relating gasoline mileage to engine displacement and number of
carburetor barrels is statistically significant.


### (3c) Calculate $R^2$ and Adjusted $R^2$



We compute the coefficient of determination $R^2$ and the adjusted coefficient of determination $R^2_{\text{Adj}}$ for the multiple linear regression model and compare them with those from the simple linear regression model relating gasoline mileage to engine displacement.


```{r}
summary(model3)$r.squared
summary(model3)$adj.r.squared

```
For the multiple linear regression model including engine displacement $x_1$ and number of carburetor barrels $x_6$, the coefficient of determination is
\[
R^2 = 0.7862,
\]
and the adjusted coefficient of determination is
\[
R^2_{\text{Adj}} = 0.7714.
\]
This indicates that approximately $78.6\%$ of the variability in gasoline mileage is explained by the model.



To compare these values with those from the simple linear regression model relating gasoline mileage to engine displacement only, we fit the model $y = \beta_0 + \beta_1 x_1 + \varepsilon$.


```{r}
model.simple <- lm(y ~ x1, data = b3)
summary(model.simple)

```

For the multiple linear regression model relating gasoline mileage to engine displacement $x_1$ and the number of carburetor barrels $x_6$, the coefficient of determination is
\[
R^2 = 0.7862,
\qquad
R^2_{\text{Adj}} = 0.7714.
\]

For the simple linear regression model relating gasoline mileage only to engine displacement $x_1$, the corresponding values are
\[
R^2 = 0.7720,
\qquad
R^2_{\text{Adj}} = 0.7644.
\]

The multiple regression model has slightly larger values of both $R^2$ and $R^2_{\text{Adj}}$, indicating a modest improvement in explanatory power when $x_6$ is added. However, the increase is small, suggesting that engine displacement alone explains most of the variability in gasoline mileage.


### (3d) Find a 95% COnfidence Interval for B1
```{r}
confint(model3, level = 0.95)

```

A 95\% confidence interval for the slope parameter $\beta_1$ is obtained using
\[
\hat{\beta}_1 \pm t_{0.975,\;n-p-1}\,SE(\hat{\beta}_1).
\]

From the model output, the 95\% confidence interval for $\beta_1$ is
\[
(-0.0656,\,-0.0405).
\]

Since this interval does not contain $0$, we conclude that $\beta_1$ is statistically different from zero at the 5\% significance level.

### (3e) Compute the $t$-statistics for testing $H_0:\beta_1=0$ and $H_0:\beta_6=0$.

```{r}
summary(model3)

```
From the model output, the $t$-statistic for testing $H_0:\beta_1=0$ is
\[
t = -8.628 \quad \text{with p-value } 1.68\times 10^{-9}.
\]
Since the p-value is much smaller than $0.05$, we reject $H_0$ and conclude that engine displacement $x_1$ is a significant predictor of gasoline mileage.

For testing $H_0:\beta_6=0$, the $t$-statistic is
\[
t = 1.387 \quad \text{with p-value } 0.176.
\]
Since the p-value exceeds $0.05$, we fail to reject $H_0$ and conclude that the number of carburetor barrels $x_6$ is not a significant predictor of gasoline mileage.


### (3f) Find a 95\% confidence interval for the mean gasoline mileage when $x_1 = 275$ in.$^3$ and $x_6 = 2$ barrels.
 


```{r}
newdata <- data.frame(x1 = 275, x6 = 2)
predict(model3, newdata, se.fit = TRUE)

```
```{r}
yhat <- 20.18723
se   <- 0.6448389
df   <- 29

tcrit <- qt(0.975, df)
c(lower = yhat - tcrit*se,
  upper = yhat + tcrit*se)

```

A 95\% confidence interval for the mean gasoline mileage when $x_1=275$ in.$^3$ and $x_6=2$ is
\[
(18.87,\ 21.51).
\]


### (3g) 95\% prediction interval for a new observation
Find a 95\% prediction interval for a new observation of gasoline mileage when $x_1=257$ in.$^3$ and $x_6=2$ barrels.


```{r}
newdata <- data.frame(x1 = 257, x6 = 2)
predict(model3, newdata, interval = "prediction", level = 0.95)

```
A 95\% prediction interval for a new observation of gasoline mileage when $x_1=257$ in.$^3$ and $x_6=2$ is
\[
(14.83,\ 27.45).
\]



### (3h) Breusch - Pagan test

The null hypothesis is that the residuals have constant variance (homoscedasticity), and the alternative hypothesis is that the residuals have non-constant variance (heteroscedasticity).


```{r}
library(lmtest)
bptest(model3)

```
The Breusch--Pagan test yields
\[
BP = 3.64 \quad \text{with p-value } 0.1617.
\]
Since the p-value is greater than $0.05$, we fail to reject the null hypothesis and conclude that there is no evidence of heteroscedasticity.

